### Template **3.2 – Transformation Pipeline Doc**

_(Phase 3 – Feature / Service Design)_

This deliverable has **two parts**:

1. **Mermaid flow diagram skeleton** – paste into a `README.md`, Confluence, or any Mermaid-enabled viewer.
2. **Step-by-step pipeline table** – copy into **Google Sheets** (_File → Import → “Paste data”_) for detailed tracking / change control.
    

---

#### 3.2-A Mermaid Flow Skeleton

```mermaid
flowchart TB
    %% RAW LAYER
    subgraph "Raw Layer"
        R1["NHANES_CSV<br/>s3://raw-data/nhanes/"]
        R2["Wearable_API<br/>Terra API"]
    end

    %% STAGING / CLEAN
    subgraph "Staging / Clean"
        S1["demographics_clean<br/>(Athena table)"]
        S2["wearable_daily_clean"]
    end

    %% FEATURE STORE
    subgraph "Feature Store"
        F1["feature_bmi"]
        F2["feature_hrv_week_avg"]
    end

    %% MODEL INPUT
    subgraph "Model Input"
        M1["model_training_view"]
    end

    %% EDGES
    R1 -->|Glue Job: nhanes_clean.py| S1
    R2 -->|Lambda: terra_cleaner| S2
    S1 & S2 -->|PySpark: feature_eng.py| F1 & F2
    F1 & F2 -->|SQL View: training_view.sql| M1

    %% CLASS ASSIGNMENTS
    class R1,R2,S1,S2,F1,M1 restricted;
    class F2 confidential;

    %% CLASS DEFINITIONS
    classDef restricted fill:#ffdddd,stroke:#cc0000,color:#660000;
    classDef confidential fill:#ffeedd,stroke:#cc6600,color:#663300;

```

> **Legend:**  
> 🟥 **Restricted** – PHI/PII; must stay encrypted and access-controlled.  
> 🟧 **Confidential** – derived metrics; still sensitive.

---

#### 3.2-B Transformation Pipeline Table (Google Sheets-ready)

|**Field**|**Description / Allowed Values**|**Example**|
|---|---|---|
|**Step ID**|Unique slug|`step_nhanes_clean`|
|**Order**|Pipeline order (1 … N)|1|
|**Source Asset ID(s)**|From Data Classification Register|`nhanes_demo_2017_20`|
|**Source Classification**|Restricted · Confidential · …|Restricted|
|**Tool / Runtime**|_Glue ETL, PySpark, Lambda, SQL, Pandas_|Glue Job|
|**Script / IaC Path**|Repo path to code / Terraform module|`etl/nhanes_clean.py`|
|**Output Dataset / Table**|Name & destination (S3, Athena, Redshift)|`demographics_clean` (Athena)|
|**Output Classification**|Inherits or downgrades?|Restricted|
|**Schedule / Trigger**|Cron, event, manual|`cron(0 2 * * ? *)` (daily @ 02:00 UTC)|
|**Data-Quality Assertions**|# tests executed (Great Expectations, Deequ…)|12 tests – **all pass**|
|**Encryption Verified**|Yes / No|Yes|
|**Separation-of-Duties**|Runner ≠ approver?|Yes|
|**Change Ticket ID**|Jira / PR link|`ETL-101`|
|**Owner**|Name & email|[hassan.iqbal@rejuve.ai](mailto:hassan.iqbal@rejuve.ai)|
|**Last Run Status**|Success · Failed · Warn|Success|
|**Notes**|Free text|Cast `RIDSTATR` to bool; remove 12 dup rows|

> **Usage Tips**

- **Version control** – Every script path **must** originate from a PR; link its ID in “Change Ticket ID.”
- **Data classification** – If output level drops (e.g., Restricted → Confidential), note the de-identification step in _Notes_.
- **Automated checks** – Integrate Great Expectations/Deequ; store JSON results in the same S3 folder for auditors.

---

**Next template:** **3.3 – Backend Service Design** (AWS architecture & endpoint spec).  
